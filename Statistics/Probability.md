### Теорема Байеса

Теорема Байеса, названная в честь пресвитерианского священника XVIII века Томаса Байеса (правильная транскрипция — Бейз), — это метод подсчёта обоснованности верований (гипотез, заявлений, предложений) на основе имеющихся доказательств (наблюдений, данных, информации). Наипростейшая версия теоремы звучит так:

**изначальная вера + новые свидетельства = новая, улучшенная вера**

Если подробнее: вероятность того, что убеждение истинно с учётом новых свидетельств равна вероятности того, что убеждение было истинно без этих свидетельств, помноженной на вероятность того, что свидетельства истинны в случае истинности убеждений, и делённой на вероятность того, что свидетельства истинны вне зависимости от истинности убеждений.

#### Формула:
$$
P(B|E) = \frac{P(B) \cdot P(E|B)}{P(E)}
$$

- **P** — вероятность  
- **B** — убеждение  
- **E** — свидетельства  
- **P(B)** — вероятность того, что **B** истинно  
- **P(E)** — вероятность того, что **E** истинно  
- **P(B|E)** — вероятность **B** в случае истинности **E**  
- **P(E|B)** — вероятность **E** в случае истинности **B**  

Самое слабое место теоремы — это **P(B)**, априорная вероятность, так как мы позволяем догадкам вкрадываться в расчёты.

---

### Условная вероятность

**Условная вероятность** — это вероятность наступления одного события при условии, что другое событие уже произошло.

- **p(x|y)** — условная вероятность события **x** при условии события **y**  
- **p(x, y)** — совместная вероятность **x** и **y**  
- **p(x)** и **p(y)** — вероятности каждого события по отдельности  

#### Формула:
$$
p(x|y) = \frac{p(x, y)}{p(y)}
$$
$$
p(x, y) = p(x|y) \cdot p(y) = p(y|x) \cdot p(x)
$$
$$
p(x|y) = \frac{p(y|x) \cdot p(x)}{p(y)}
$$

---

### Применение теоремы Байеса в машинном обучении

Перепишем теорему Байеса в контексте машинного обучения:

#### Формула:
$$
p(\theta|D) = \frac{p(\theta) \cdot p(D|\theta)}{p(D)}
$$

- **D** — данные, выборка  
- **θ** — параметры модели, которые мы хотим обучить  
- **p(θ)** — априорная вероятность (*prior probability*), она является математической формализацией нашей интуиции о предмете, формализацией того, что мы знали раньше, ещё до всяких экспериментов.  
- **p(θ|D)** — апостериорная вероятность (*posterior probability*), то, что мы хотим найти: распределение вероятностей параметров модели после учёта данных.  
- **p(D|θ)** — правдоподобие (*likelihood*), вероятность данных при условии зафиксированных параметров модели. Это обычно находят легко, так как конструкция модели заключается в задаче функции правдоподобия.  

---

### Другие понятия

- **Если два события независимы**, то их произведение равно произведению их вероятностей.  
- **Полная вероятность** — используется для расчёта вероятности наступления события с учётом всех возможных факторов.  
- **Статистическая модель перевода** обучается за счёт большого количества параллельных текстов (например, тексты на русском и на английском), таких как корпус Европарламента, заседания которого переводились на все языки ЕС.  

---

### Расстояние Левенштейна

**Расстояние Левенштейна** (также редакционное расстояние или дистанция редактирования) между двумя строками в теории информации и компьютерной лингвистике — это минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую.

---

### Вероятностное моделирование

**LDA (Latent Dirichlet Allocation)** — модель для выделения тем в корпусе документов без использования размеченных данных, где один документ может содержать несколько тем.

**Прочие понятия:**  
- **Метод наибольшего правдоподобия** (*Likelihood Estimation*)  
- **NER (Named Entity Recognition)** — распознавание именованных сущностей  
- **Сэмплирование в вероятностных моделях** — один из основных методов приближённого вывода  

---

### Ряд Тейлора

Ряд Тейлора — это разложение функции в окрестности точки, позволяющее аппроксимировать функции с помощью многочленов.

---

### Системы и инструменты:

- **Мозес** и **Систран** — системы машинного перевода.