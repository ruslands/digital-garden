Siamese NNs are popular among tasks that involve finding similarity or a relationship between two comparable things.

cyclicLR, rediceonplateu, SWA, statification

Object Detectors: RetinaNet, SSD, YOLO, R-FCN, Fast-RCNN, Faster-RСNN

Было как-то в другой задаче при переходе с кераса на пайторч, воспроизводимость результатов сильно исправила инициализация весов через kaiming_normal.

Every ML algorithm has 3 components

- Representation

- Evaluation

- Optimization

Representation:

Decision Trees

SVM

Neural Network

Graphical models (Bayes Markov nets)

Instances

Evaluation:

Accuracy

Precision Recall

Squared Error

Likelihood

Posterior Probability

Cost/Utility

Margin

Entriopy

K-L divergence

Optimization:

Combinatorial optimization (Greedy search)

Convex optimization (Gradient Descent)

Constrained optimization (Linear programming)

Байт код(Python,Java) - исполняет виртуальная машина языка(JVM), потом передает команды процессору.

Машинный код(С,С++) - код, который исполняется напрямую процессором компьютера

JIT - для ускорения работы байт-кода.

У Python есть реализация(виртуальная машина) их много - cPython,Cython,Jython,PyPy

VM - Транслирует код в байт-код, а затем интерпретирую его дает команды процессору

GIL

Перцептрон

Многослойный Перцептрон Розенблата

Многослойный Перцептрон Румельхарта учиться с помощью метода обратного распространения ошибки

Алгоритмы:

Детектор границ Кэнни - делает 5 шагов по изображению.

Преобразование Хафа - определение прямых на изображении, определение четырехугольных объектов.

Вычисление Гистограмм: определить передвижение, выделить фон

Тест тьюринга

graphics.cs.msu.ru

tevian.ru

Резонный вопрос — а нельзя ли натренировать детектор и посчитать потом баундинг боксы каждого класса? Ответ — можно. Некоторые парни так и сделали. Александр Буслаев (13-ое место) натренировал SSD, а Владимир Игловиков (49-ое место) — Faster RCNN.

Минус такого подхода состоит в том, что он сильно ошибается, когда сивучи на фотографии очень плотно лежат друг к другу. А наличие нескольких различных классов еще и усугубляет ситуацию.

Решение, основанное на сегментации с помощью UNet, тоже имеет место быть и вывело Константина на 2 место.

решение основано на UNet. Опишу его в двух словах. Парень разметил вручную сегментационные маски для 3 изображений, обучил UNet и предсказал маски на еще 100 изображениях. Поправил маски этих 100 изображений руками и заново обучил на них сеть.

Для получения числа особей по маскам он использовал морфологические операции и детектор блобов.

Гистограмма - график расределения пикселов по яркости. Горизонтальная ось(0,255) - шкала яркости от черного до белого, вертикальная ось - кол-во пикселов соответствующей яркости.

jpeg - алгоритм сжатия с потерями.

Малоконтрастность - пикселы из изображения принимают значения не из всего диапазона [0-255]

Коррекция изображений:

 - Линейная коррекция

   - Линейное растяжение гистограммы - компенсация узкого диапазона яркостей.

 - Нелинейная коррекция

Классификация изображений

- Бинарная классификация

- Многоклассовая классификация

Свертка изображения - накладывание на изображение ядра(прямоугольная таблица с числами), затем значения суммируются и записываются в выходную матрицу.

Медианный фильтр - рассматривает окрестность изображения и ищет выбросы.

Ways to image classification

KNN -

The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.

Classifying a test image is expensive since it requires a comparison to all training images.

NN approach have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels.

Softmax Classifier (Multinomial Logistic Regression) Want to interpret raw classifier scores as probabilities

Multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems

Multinomial logistic regression is known by a variety of other names:

- including polytomous LR

- multiclass LR

- softmax regression

- multinomial logit

- maximum entropy (MaxEnt) classifier

- conditional maximum entropy model

One vs All asks - if I compare the subjects who responded XXXX to all others, what can I say?

Multinomial asks - What can be said about the differences among the people who respond at each level?

The performance difference between the SVM and Softmax are usually very small

Backpropagation is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks.

Fast Region-based Convolutional Network method (Fast R-CNN)

Отклик фильтра использовать как признак пиксела.

Извлечение признаков - разбить изображение на области и вычислять гистограмму по областям.

Признаковое описание через HOG

Features -> Coding -> Spatial Pooling

Spatial Pooling(Пространственная аггрегация) = выбор областей для аггрегации и метод аггрегации.

Мешок слов в изображениях

Построение вектора-признака изображения делят на 3 этапа.

- Вычисление признаков пикселов.(Features)

- Кодирование признаков.(Coding)

- Пространственная аггрегация.(Spatial Pooling)

Для кодирования и аггрегации использует - гистограммы.

Вектор-признак это гистограмма

Сравнение гистограмм - Histogram intersection, L1 distance, x^2distance, Quadratic distance

ImageNet - 1000 изображений на 117 тыс.классов

В качестве признаков используют гистограммы распределения цветов, градиентов, краев, визуальных слоев.

Любая булева функция представима в виде ДНФ, следовательно, и в виде двухслойной сети.

Для bounding box - нужно 4 выхода нейросети

Полносвязный слой - все нейроны это слоя связаны со всеми нейронами предыдущего слоя.

Метод обратного распространения ошибки - название реализации градиентного спуска для нейросети

Функции активации НС

Tanh - Гиперболический тангенс

Sigm - Сигмоида [0,1]

Rectified Linear(ReLu) - max(0,x)

Функции потерь

Функция потерь для многоклассовой классификации (2+ нейрона)

Softmax Layer -> Cross-Entropy

Softmax Layer - Для того, чтобы выход нейросети удовлетворял условим распределения т.е. сумма весов равна единице и каждый вес менялся от 0 до 1, то на выходе нейросети ставится специальный слой Softmax-преобразование. Преобразует вектор вектор длины n в вектор в котором все значения распределены от 0 до 1, а сумма значений равна 1

Cross-Entropy - сравнивает распределение вероятностей. Столько выходов сколько классов, на выходе набор вероятностей сумма, которых равна 1. К примеру есть три класса: истинный классы 0,1,0 нейросеть выдал 0.2, 0,6, 0.2 В качестве функции ошибки нужно выдать результат сравнения этих двух распределений. Это помогает сделать кросс-энтропия, сравнивает распределения. E-y_true * Logy_pred

Функция потерь для бинарной классификации (1 нейрон)

Hinge loss - Lhinge = max(0, 1 - y_pred * y_true)

Как обучать НС

Метод градиентного спуска, который выродился в метод обратного распространения ошибки(BackPropogation).

Расчет градиента для нейросети это тажа самая задача, что и расчета градиента(дифферинцирование) сложной функции. Можем применять прафила дифферинцироания сложных функций к нейронным сетям.

Если применить правило дифферинцирования сложной функции для НС, то процедура вычисления градиента становится похожа на запуск нейросети в обратном направлении. Пример подается на вход нейросети, запоминаем выходы каждого нейрона, на выходе вычисляем ошибку, затем вычисляем градиент ошибки нейронов, которые связаны с выходом, далее считаем градиент ошибки для предыдущих нейронов и так пока не придем в начало.

Сейчас все используют MinibatchSGD

Вместо того, чтобы вычислять ошибку по каждому примеру и менять параметры, накапливают несколько примеров

Это делают потому что градиент получется шумным, в случае minibatch идет усреднение.

Начальное приближение - инициализация сети, подбор параметров.

Расписание - процесс формирования minibatch

В minibatch должны присутствовать положительные и отрицательные примеры

Проблемы обучения:

Затухание градиентов - если попасть в область низкого градиента функции активации, то веса соответствующий этому градиенту изменяются очень медленно.

Sigmoid - подвержена этому

ReLu - не подвержена

Мертвые нейроны - нейроны, которые получают только отрицательные или нулевые входы

Исчезающие и взрывные нейроны

Человек использует информацию о краях и градиентах изображения, на этом основаны НС,

в нейросятех нужны линейные фильтры, чтобы НС могла извлекать информацию о градиентах.

После чего можно слабые градиент подавить, а сильные подчеркнуть и получить набор признаков основанных на градиентах изображений.

Инвариативность - неизменяемое

Нейроны реализуют операцию свертки, организуют группу нейронов в сверточный слой, каждый нейрон связан с группой входов, если нужно находить градиенты изображений нужно реализовать несколько сверточных слоев

Нейронов получается много, а число параметров небольшое

С помомощью сверточного слоя можно реализовать поиск градиентов изображения.

Max-Pooling - применяем фильтр max к некоторой окрестности группе простых клеток.

Несколько клеток находят положение края в разных местах, затем берем от них максимиум

Сверточные сети = неокогнитрон + метод обратного распространения ошибки

каждый из нейронов подсоединен только к небольшой окрестности на изображении.

ядро свертки – это совокупность весов этого нейрона.

Первый плюс, что меньше весов, быстрее считать, меньше подвержены к переобучению. С другой стороны, каждый из этих нейронов получается некоторым детектором, как я покажу это дальше. Допустим, если где-то у нас на изображении есть глаз, то мы одним и тем же набором весов, пройдясь по картинке, определим, где на изображении глаз.

Эта операция свертки элементарная, и мы строим слои этих сверток над изображением, преобразовывая все дальше и дальше изображения. Таким образом, мы на самом деле получаем новые признаки.

Наши первичные признаки – это были пиксели, а дальше мы преобразовываем изображение и получаем новые признаки в новом пространстве, которые, возможно, позволят нам более эффективно классифицировать это изображение. Если вы себе представите изображение собак, то они могут быть в самых разных позах, в самых разных освещениях, на разном фоне, и их очень сложно классифицировать, непосредственно опираясь только на пиксели. А последовательно получая иерархию признаков новых пространств, мы это сделать можем.

Subsampling

Сгенерировать изображение, которое дает тот же самый вектор признак, что и исходное изображение.

Полносвязные слои сохраняют информацию о семантике(какого рода объекты на изображении), но игнорируют информацию о местоположении объекта на изображении.

поменять классификатор - замена последних слоев

Дообучение нс

Первые слои извлекают простую информацию в которой семантики нет - края, границы, текстуры

Сверточная + рекурентная сети решает задачу - картинка в текст

/-----------Перенос стиля--------/

Prisma

Содержимое изображения и стиль разделимы

Верхние слои больше описывают содержания изображения

Нижние слои стиль изображения.

Нейропластичность — это свойство мозга изменяться под воздействием опыта или после травмы.

Если вы знакомы с предобучением глубоких сетей с помощью автоенкодеров или ограниченных машин Больцмана,

Свертка

Активация

Пулинг или субдескритизация

Полносвязная нейронная сеть

для предобучения обыкновенной многослойной сети прямого распространения. Такая сеть обычно обучается алгоритмом обратного распространения ошибки,

Галушкин Синтез многослойных систем распознавания образов

Кол-во сверток = кол-во выходных слоев

Свертка 1х1 изменяет глубину изображения.

Ограниченная машина Больцмана (англ. Restricted Boltzmann machine), сокращенно RBM — вид генеративной стохастической нейронной сети, которая определяет распределение вероятности на входных образцах данных.

CycleGAN

FC - full connected layer

ImageNet Challenge

LeNet - LeCun 1998

AlexNet - 2012,

VGG

GoogLeNet - особенность, делает свертки изображения с разным ядром. Нет полносвязных слоев. Есть inception module - слой берущий фичи на разных разрешениях, соеденили такие слои вместе

AlexNet - в честь алексея, это тип архитектуры

Spatial Pyramid Pooling(SPP) - слой, сохраняет пространственную информацию

Average pooling - слой, который не сохраняет пространственную информацию, когда не важно знать где нашелся признак, важнее знать, что он есть. В рез-тате получиться вектор признак длиной равной числу сверток на последнем слое.

VGG - архитектура нс, рост качества за счет глубины НС. используют только маленькие свертки 3х3, используют stride = 1(шаг), ReLu активация, кол-во сверток пропорционально уменьшению изображения, замена больших сверток на несколько маленьких 5х5 замена на 2 3х3, получается меньше параметров для обучения, если 5х5 - то 25 параметро, 2х3х3 = 18 параметров. Но тем не менее требует много памяти т.к. много параметров для обучения.

Network in Network - 

SqueezeNet - использует свертку 1х1 для уменьшения глубины нейросети.

ZeroPading - слой

Inception -

ResidualNet(ResNet) - остаточная сеть с обходными путями, решает проблему падения кач-ва при добавлении слоев в НС, будем изучать не преобразование данных, а изменение данных. Изучать преобразование - это обучать параметры этого преобразования

Песочные часы - архитектура НС с различными путями.

Большие свертки можно приближать последовательностью сверток 3х3;

Свертки 1х1 позволяют управлять толщиной слоя и уменьшать вычислительную сложность.

Residual - связи позволяет снизить проблему затухания градиента и обучать очень глубокие НС.

Детектор лиц

Каскад нейросетей - серия классификаторов, где некоторые простые классификатор отбрасывают области, где точно нет лиц. Затем сложные классификатор ищут лица на оставшейся области.

Faster R-CNN - имеет простую НС, которая строит гипотезы, затем работает сложная НС на тех областях, на которых простая НС сделала гипотезу.

Watch-List - есть список подозрительны лиц, определить, входит ли человек в этот список по его фотографии. Сложная постановка задачи.

метрика Махаланобиса - измеряет расстояние между векторами-признаками

метрика Triplet Loss

\----------------\

Слой - нейроны и входящие в них ребра.

Скрытый слой потому что мы не наблюдаем значения скрытых нейронов явно

Берем слой и подаем производные функции потерь по его выходу он отдает функцию потерь по своему входу и так далее.

Начальное приближение берем с нормальным распределением и маленькой дисперсией, еще есть инициализация Xavier

Модель имеет большой bias(смещение) - значит модель недостаточно обобщающая.

Модель имеет большой variance(дисперсия) - значит модель переобучилась.

Dropout - помогает бороться с переобучением, зануление случайных выходов(выключение нейронов), на каждой итерации обучения.

Функция потерь подбирается исходя из задачи.

Для регресии используют MSE.

Hinge loss, ступенчатая функция

Max pooling/Average pooling - помогает уменьшать размерность, т.к. число слоев свертки растет линейно, если нужно обработать большое изображение потребуется много слоев и это нехорошо. Делим изображение на непересекающиеся области к примеру 2х2 и берем макисмальное/среднее значение из этой области и записыаем в новую матрицу, ставят между слоями свертки.

аугментация

Инициализация важна, должна быть не 0

BatchNorm(BN) - слой, который нормализует значения каждого выходного слой свертки

методов augmentations - то есть искажения исходного изображения по какому либо паттерну),

Для детектирования

R-CNN

Fast-R-CNN

Faster-R-CNN

YOLO

R-CNN - есть selective search, который генерирует много bounding box около 2000, затем каждый bb прогоняем через свертучную сеть, которая может быть обучена на другом датасет. Данное решение очень долго работает.

Selective search - сегментирует изображение на основе яркости пикселей. Этот алгоритм не связан с машинным обучением. Проблема - может пересегментировать, трудно подобрать порог.

Fast-R-CNN - выделяет регионы не на картинке как делает R-CNN, а на карте признаков, которая получена после сверточных слоев.

Faster-R-CNN - вместо selective search сделана region proposal network. Выделяет потенциальные bounding box

YOLO(you only look once) - использует non-max supression

ROI-pooling - модификация max-pooling,

FCN(fully convolution network) - семантическая сегментация, сети состоят только из сверточных и пулинговых слоев,

/--NN for segmentation--/

SegNet -

UNet -

Mask R-CNN -

/--Tracking--/

High-Speed Tracking with Kernelized Correlation Filters

Резонный вопрос — а нельзя ли натренировать детектор и посчитать потом баундинг боксы каждого класса? Ответ — можно. Некоторые парни так и сделали. Александр Буслаев (13-ое место) натренировал SSD, а Владимир Игловиков (49-ое место) — Faster RCNN.

Минус такого подхода состоит в том, что он сильно ошибается, когда сивучи на фотографии очень плотно лежат друг к другу. А наличие нескольких различных классов еще и усугубляет ситуацию.

Решение, основанное на сегментации с помощью UNet, тоже имеет место быть и вывело Константина на 2 место.

решение основано на UNet. Опишу его в двух словах. Парень разметил вручную сегментационные маски для 3 изображений, обучил UNet и предсказал маски на еще 100 изображениях. Поправил маски этих 100 изображений руками и заново обучил на них сеть.

Для получения числа особей по маскам он использовал морфологические операции и детектор блобов.

Причем, чтобы иметь пониженную ставку налогов для IT-компаний, вам нужно иметь ОКВЭД 72.20 Разработка программного обеспечения и консультирование в этой области и аккредитоваться в «Росинформтехнологиях».

Также есть ограничение, куда можно потратить полученные деньги. Это целевое финансирование на НИОКР. Мы тратим на разработку (зарплата) и аренду помещения. На маркетинг и продвижение, например, тратить их нельзя.

Вся отчетность ведется в онлайн системе, которая называется незамысловато – ФОНД.

Общее отношение к вопросу привлечения капитала доходчиво описано в книге Гая Кавасаки «СТАРТАП».

Семейство методов Монте-Карло представляет собой собрание разнообразных методов и моделей, объединенных идеей проведения большой серии стохастических вычислительных экспериментов с последующей статистической обработкой их результатов

/---UNet---/

Unet network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.

Unet feature map?

Larger patches require  more  max-pooling  layers  that  reduce  the  localization  accuracy, while small patches allow the network to see only little context.

Another challenge in many cell segmentation tasks is the separation of touching objects of the same class; see Figure 3. To this end, we propose the use of

a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function.

ISBI cell tracking challenge 2015

map  with  a  pixel-wise  loss  weight  to  force  the network to learn the border pixels.

ground truth segmentation.

/--------Обработка изображений---------/

Peak signal-to-noise ratio (Пиковое отношение сигнал / шум) - PSNR наиболее часто используется для измерения уровня искажений при сжатии изображений.  
Для цветных изображений с тремя компонентами RGB на пиксель применяется такое же определение PSNR, но MSE считается по всем трем компонентам (и делится на утроенный размер изображения).

Следует отметить, что термин «Пиковое отношение сигнала к шуму» является часто употребляемым, но не совсем верным дословным переводом английского термина «peak signal-to-noise ratio». Правильным переводом будет являться «отношение пикового уровня сигнала к шуму». Здесь учитывается тот факт, что при вычислении PSNR вычисляется именно отношение максимально возможного («пикового») сигнала по отношению к уровню шума, а не ищется максимальное («пиковое») отношение вычисленного значения сигнал/шум, как можно было бы понять из неверного дословного перевода.

Object Contour Detection  
object localization  
Object segmentation

Логарифмическая шкала - начало единица

[https://arxiv.org/abs/1603.04530](https://arxiv.org/abs/1603.04530)

[http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shen_DeepContour_A_Deep_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shen_DeepContour_A_Deep_2015_CVPR_paper.pdf)

Математическое ожидание - это параметр генеральной совокупности - такой бесконечной кучи чисел. Раз куча бесконечная, то значение математического ожидания никогда никому не будет известно. А среднее значение - это попытка вычислить математическое ожидание по выборке из генеральной совокупности.  
   
Ну, а если строго, то среднее значение - это точечная оценка математического ожидания.

Нейронные сети позволяют находить сложные нелинейные разделяющие поверхности, благодаря чему широко используются в таких трудных задачах, как распознавание изображений и речи. В этом модуле мы изучим многослойные нейронные сети и их настройку с помощью метода обратного распространения ошибки.

Нейронная сеть это композия линейных моделей классификации и регресии

w0 - порог активации  
функции активации:

- Сигмоидная функция  
- Гиперболический тангенс (Т)  
- Логарифмическая функция (L)  
- Гауссовская функция (G)  
- Линейная функция  
- Пороговая функция Хевисайде

Обычно достаточно 3 слоев НС  
Deep Learning это более 1 скрытого слоя

Самый известный способ обучения многослойных НС - метод обратного распространения ошибки

Скрытый слой НС называется потому что он находится между слоем входных значений и слоем выходных значений

У каждого нейрона может быть своя функция активации

этот алгоритм был объявлен «лучшим в мире» после прохождения тестов NIST и MegaFace.  
В мае 2017 г. стало известно, что «Вокорд», наряду с российскими компаниями NtechLab и 3DiVi,

На собеседовании вам точно встретятся:  
- дифференцирование;  
- максимальное правдоподобие;  
- дерево решений и случайный лес;  
- градиентный спуск;  
- экспоненциальное семейство распределений;  
и ими дело, конечно же, не ограничится.  
Подсказка: чтобы отлично пройти собеседование внимательно изучите курсы CS229, CS231n, CS224d.

Классические нейронные сети - сети прямого распространения(feedforward network) не обладают памятью  
'Активации' проходят всю сеть насквозь  
Рекуррентные сети - нейросети с памятью, запоминает предыдущее состояние  
За счет наличия обратных связей 'активации' циркулируют в сети

LSTM нейроархитектура относится к рекурентным нейросетям

Перцептрон может реализовывать разные функции, всего навсего меняя свои веса.

Численные методы - в математике, методы приближённого решения математических задач, сводящиеся к выполнению конечного числа элементарных операций над числами.

Выбираем функцию, сводим обучение нейронной сети к минимизации этой функции, минимум функции ищем используя градиентный спуск

Метод Монте-Карло - численный метод

Перцептрон не может реализовать XOR, потому что он линеен.

Gradient Descent  
Используется для поиска минимума функции

Gradient Descent minimizes a function by following the gradients of the cost function.

/----------Численные методы для поиска корня функции----------/  
Метод деления пополам или дихотомия.  
Для поиска корня функции, постоянно делим на 2 промежуток в котором находится корень

Метод Ньютона(Метод Касательных) -  если — некоторое приближение к корню уравнения , то следующее приближение определяется как корень касательной к функции , проведенной в точке  

Метод Секущих  
Метод Парабол  
Метод Простых Итераций