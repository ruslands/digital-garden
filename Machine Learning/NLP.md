/---------------NLP (Обработка естественного языка)-----------------/

Разделы знаний о языке:  
Фонетика  
Морфология - изучает формы слов. Словоизменение и Словообразование.  
Синткасис  
Семантика  
Прагматика

NLP используется для:

- Извлечения информации  
- Анализ тональности  
- Автоматическое реферирование  
- Обучение языку  
- Диалоговые системы  
- Вопросно-ответные системы  
- Информационный поиск  
- Машинный перевод

Модели  
 - Конечные автоматы (FSA)  
 - Вектороное представление  
   
 /-------------SPARK--------------/  
Udemy*

NLP basic process  
1. Compile all documents (Corpus)  
2. Featurize the words to numerics  
3. Compare features of documents

CountVectorizer  
   
   
/----Тематическое моделирование (Topic Modeling)----/

Тематическое моделирование решает классическую задачу анализа текстов – как создать вероятностную модель большой коллекции текстов, которую затем можно будет использовать, например, для информационного поиска (information retrieval), классификации, рекомендаций контента.  

В модели наивного байесовского классификатора каждому документу присваивается скрытая переменная, соответствующая его теме (например «финансы», «культура», «спорт»)  
   
- Одно из направлений обработки естественного языка  
- Разновидность статического анализа текстов  
- Технология поиска информации не по словам, а по смыслу  
- Выявление скрытых интересов по наблюдаемым данным  
- Кластеризация текстовых документов  
- Модель машинного обучения без учителя

Тема в коллекции текстовых документов  
Тема - семантически однородный кластер текстов; специальная терминология предметной области; набор часто совместно встречающихся терминов

Тема - Условное распределение на множестве терминов. Частотный словарь.  
p(w|t) - вероятность(частота) термина w в теме t.

Тематика документа - Условное распределение.Какие темы встречаются в этом документе.  
p(t|d) - вероятность(частота) темы t в документе d.

Когда автор писал термин w в документе d, он думал о теме t, и мы хотели бы выявить, о какой именно.

Тематическая модель выявляет латентные(скрытые) темы по наблюдаемым распределениям слов p(w|d) в документах.

/------Семантический анализ------/

term frequency-inverse document frequency частотность терминов-обратная частотность документов  
Это простой и удобный способ оценить важность термина для какого-либо документа относительно всех остальных документов. Принцип такой — если слово встречается в каком-либо документе часто, при этом встречаясь редко во всех остальных документах — это слово имеет большую значимость для того самого документа.  
- Слова, неважные для вообще всех документов, например, предлоги или междометия — получат очень низкий вес TF-IDF (потому что часто встречаются во всех-всех документах)  

TF  — это частотность термина, которая измеряет, насколько часто термин встречается в документе. Логично предположить, что в длинных документах термин может встретиться в больших количествах, чем в коротких, поэтому абсолютные числа тут не катят. Поэтому применяют относительные — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в тексте.  

IDF — это обратная частотность документов. Она измеряет непосредственно важность термина. То есть, когда мы считали TF, все термины считаются как бы равными по важности друг другу. Но всем известно, что, например, предлоги встречаются очень часто, хотя практически не влияют на смысл текста. И что с этим поделать? Ответ прост — посчитать IDF. Он считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин а.  

А затем мы умножаем TF на IDF и получаем — барабанная дробь — TF-IDF!  

import gensim

word2vec — это инструмент (набор алгоритмов) для расчета векторных представлений слов, реализует две основные архитектуры — Continuous Bag of Words (CBOW) и Skip-gram.  
На вход подается корпус текста, а на выходе получается набор векторов слов.  

Для обучения модели word2vec хорошего качества нужен большой-пребольшой корпус

Многие считают, что word2vec — это алгоритм глубокого обучения (которое Deep Learning).  
Спешу расстроить — word2vec это не глубокое обучение.  
Потому как там применяется вполне себе обычная, а не «глубокая» нейросеть прямого распространения (Feed-forward Neural Network).  

Хотим учитывать порядок слов  
Семантическая близость  
Лексическая близость

Стоп слова не стоит выкидывать  
Токенизация - разбиение текста на абзацы, либо предложения, либо слова

Word2Vec обучается вне зависимости от того размеченная выборка или нет(есть target или нет), для обучения важно большое кол-во данных

Word2Vec - нужно передавать массив с массивами слов. Т.е. мы разбиваем текст на предложения, предложения разбиваем на слова

Мы подаем слова без пропусков, алгоритм сам пропускает и обучается и пытается предсказать

Word2vec имеет 2 алгоритма  
- предсказывает пропущенное слово по контексту (scip-gram)  
- по слову предсказывает контекст, но не предсказывает позитивный или негативный отзыв (continious bag of words)

Размер окна — для Skip-gram оптимальный размер около 10, для CBOW — в районе 5  

Применение суб-сэмплирования улучшает производительность. Рекомендуемый параметр субсэмплирования от 1e-3 до 1е-5  
Субсэмплирование — это процесс изъятия наиболее частотных слов из анализа, что ускоряет процесс обучения алгоритма и способствует значительному увеличению качества получающейся модели.  

Важно не путать процесс векторизации и предикта  
word2vec - процесс векторизации

Но в общем случае лучше применять схему skip-gram + negative sampling + окно 10 слов + субсэмплирование 1е-5 + размер вектора 300,

LSTM сеть - по трем словам предсказывает следующее слово

/-------------------------------------------------/

sudo pip install -U nltk

import nltk  
from nltk import word_tokenize,sent_tokenize  
nltk.download('punkt')  
nltk.download('stopwords')

pickle  
Natural Language Toolkit  
Social Network Analysis

Лемма  
Морфема  
Граммпма  
Парадигма  
Омонимия - случайное совпадение слов (Замок, коса)  
Анафора - отсылка назад (Купил телефон, он не понравился)  
Токенизация  
Стеммер - отсекает окончания и суффиксы (красивые - красив)  
Лемматизатор - приводит к нормальной форме (красивые - красивый)  
Коллокация - называется словосочетание, имеющее признаки синтаксически и семантически целостной единицы

PLSA (Probabilistic Latent Semantic Analysis) — это тематическая модель без регуляризаторов.  
LDA (Latent Dirichlet Allocation) — это тематическая модель, в которой каждая тема сглажена одним и тем же регуляризатором Дирихле.  

t-sne - нелинейное снижение размерности

Text Categorization  
Text Classification  
Word Embedding  
word2vec  
LSTM  
Topic model  
Matrix Factorization  
Nonnegative Matrix Factorization

CountVectroizer - возвращает количество вхождения слова в тексте

TfidfVectorizer - возвращает значимость каждого слова по TF-IDF

• stop-words – список слов, которые не будут учитываться при векторизации;  
• token_pattern – регулярное выражение, по которому строка разбивается на токены. Обычно это просто разделение на слова, но могут быть выделены и другие сущности;  
• max_df – токены имеющие частотность выше этого значения не будут учитываться. Можно указать процент через коэффициент – 0.9 будет обозначать что 10% наиболее частых слов будут отброшены;  
• min_df – токены имеющие частотность ниже этого значения не будут учитываться. Можно указать процент через коэффициент – 0.1 будет обозначать что 10% наиболее редких слов будут отброшены.  

perform - выполнять  
substitution - замена

pymorphy2 - морфолигический анализатор/генератор, используют FSA (Finit State Automat)  
aot -  
mystem - морфолигический анализатор/генератор, лемматизатор/стеммитизатор  
stemka - статистический стеммер

Finite State Transducer(FST) - метод для английской морфологии

Методы морфологического анализа:  
- Сгенерировать все данные и по ним искать. Т.е. к каждому слову прописать лемму  
- Обобщить типичные случаи, прописать правила

Алгоритм Портера - стеммер для английского языка(в английском меньше вариативность).

Омонимы — одинаковые по звучанию и написанию, но разные по значению слова.  
Наречие - признак действия. Отвечает на впрос КАК.  

НКРЯ - в этом корпусе снята морфологическая омонимия(неоднозначность).  
ГКРЯ  
OpenCorpora

Также морфологическая омонимия снимается с использованием Скрытых Марковских Моделей (HMM, hidden markov model)  
НММ применяются в разных задачах ОЕЯ (языковые модели, распознование речи, генерация текста)

/-------FastText facebook-------/

FastText, for Word Representations and Text Classification.

A single word with the same spelling and pronunciation (homonyms) can be used in multiple contexts and a potential solution to the above problem is computing word representations.

Word Embeddings - Представление слов  
Word Embedding format generally tries to map a list of word to a vector  
Word Embeddings are Word converted into numbers

1. Frequency based Embedding  
Count Vector  
TF-IDF Vector  
Co-Occurrence Vector

--------------------------------------  
﻿perform - выполнять  
substitution - замена  
clarify - прояснить  

pymorphy2 - морфолигический анализатор/генератор, используют FSA (Finit State Automat)  
mystem - морфолигический анализатор/генератор, лемматизатор/стеммитизатор  
aot -  
stemka - статистический стеммер

Finite State Transducer(FST) - метод для английской морфологии

Методы морфологического анализа:  
- Сгенерировать все данные и по ним искать. Т.е. к каждому слову прописать лемму  
- Обобщить типичные случаи, прописать правила

Алгоритм Портера - стеммер для английского языка(в английском меньше вариативность).

Омонимы — одинаковые по звучанию и написанию, но разные по значению слова.  
Наречие - признак действия. Отвечает на впрос КАК.  

НКРЯ - в этом корпусе снята морфологическая омонимия(неоднозначность).  
ГКРЯ  
OpenCorpora

Также морфологическая омонимия снимается с использованием Скрытых Марковских Моделей (HMM, hidden markov model)  
НММ применяются в разных задачах ОЕЯ (языковые модели, распознование речи, генерация текста)

/-------Word Embeddings (Представление слов)-------/

Word Embedding format generally tries to map a list of word to a vector (Word converted into numbers)

1. Frequency based Embedding

-Count Vector  
-TF-IDF Vector  
-Co-Occurrence Vector (Совхождение, смежность)

/--Co-Occurrence Vector--/  
Similar words tend to occur together and will have similar context for example – Apple is a fruit. Mango is a fruit.  

2. Prediction based Embedding  
-Word2Vec (CBOW, Skip-gramm)