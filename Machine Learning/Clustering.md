/------Без учителя-------/  
- Кластеризация  
- Снижение размерности (PCA, t-SNE)  
- Manifold Learning

/------Метрика-------/  
Для кластеризации важен выбор метрики

- метрика Минковского  
- Косинусное расстояние между векторами  
- расстояние Хэмминга

Метод локтя(elbow method) для оценки числа кластеров

/-----Алгоритмы кластеризации-----/  
K-means  
C-means  
Affinity propagation(Сходство)  
Spectral clusterisation  
Aglomerative clusterisation  
DBSCAN  
EM - алгоритм  
Birch  
Ward  
MeanShift

/--K-means--/  
Алгоритм:  
1. Выбрать количество кластеров k, которое нам кажется оптимальным для наших данных.  
2. Высыпать случайным образом в пространство наших данных k точек (центроидов).  
3. Для каждой точки нашего набора данных посчитать, к какому центроиду она ближе.  
4. Переместить каждый центроид в центр выборки, которую мы отнесли к этому центроиду.  
5. Повторять последние два шага фиксированное число раз, либо до тех пор пока центроиды не "сойдутся"

Особенности алгоритма:  
Нужно заранее задавать число кластеров - помогает метод локтя(elbow method) для оценки числа кластеров  
Чувствителен к исходному положению центроид кластеров в пространстве.  
В такой ситуации спасает несколько последовательных запусков алгоритма с последующим усреднением полученных кластеров.  
Решение задачи K-means NP-трудное, поэтому иногда используется K-means miniBatch

/--Выбор числа кластеров--/  
from sklearn.cluster import KMeans  
inertia = []  
for k in range(1, 8):  
    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)  
    inertia.append(np.sqrt(kmeans.inertia_))

plt.plot(range(1, 8), inertia, marker='s');  
plt.xlabel('$k$')  
plt.ylabel('$J(C_k)$');

/--Affinity Propagation( метод распространения близости/сходства)--/  
Не требует заранее определять число кластеров.  
Идея алгоритма заключается в том, что нам хотелось бы, чтобы наши наблюдения кластеризовались в группы на основе того, как они "общаются", или насколько они похожи друг на друга.

/--Спектральная кластеризация--/  
Для работы этого алгоритма требуется определить матрицу похожести наблюдений (adjacency matrix).

/--Агломеративная кластеризация--/  
Без фиксированного числа кластеров.  
Алгоритм:  
1. Начинаем с того, что высыпаем на каждую точку свой кластер  
2. Сортируем попарные расстояния между центрами кластеров по возрастанию  
3. Берём пару ближайших кластеров, склеиваем их в один и пересчитываем центр кластера  
4. Повторять п. 2 и 3 до тех пор, пока все данные не склеятся в один кластер

Выбор числа кластеров:  
По итогам выполнения такого алгоритма можно также построить замечательное дерево склеивания кластеров (дендраграмма) и глядя на него определить, на каком этапе нам было бы оптимальнее всего остановить алгоритм. Либо воспользоваться тем же правилом локтя, что и в k-means.

Недостаток алгоритма:  
Нужно пересчитывать расстояния каждый раз после склеивания, что сильно снижает вычислительную сложность алгоритма.

/--DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума)--/  
Оперирует плотностью данных.  
   
/--------Метрики качества кластеризации--------/  
Выделяют внешние и внутренние метрики качества.  
Внешние используют информацию об истинном разбиении на кластеры  
Внутренние метрики не используют никакой внешней информации и оценивают качество кластеризации, основываясь только на наборе данных.  
Оптимальное число кластеров обычно определяют с использованием внутренних метрик.

'ARI': metrics.adjusted_rand_score(y, algo.labels_),  
'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_),  
'Homogenity': metrics.homogeneity_score(y, algo.labels_),  
'Completeness': metrics.completeness_score(y, algo.labels_),  
'V-measure': metrics.v_measure_score(y, algo.labels_),  
'Silhouette': metrics.silhouette_score(X, algo.labels_)}))

/--Adjusted Rand Index (ARI)--/  
Предполагается, что известны истинные метки объектов.  
Данная мера не зависит от самих значений меток, а только от разбиения выборки на кластеры.  
Пусть — число объектов в выборке.  
Обозначим через — число пар объектов, имеющих одинаковые метки и находящихся в одном кластере, через — число пар объектов, имеющих различные метки и находящихся в разных кластерах.  

Тогда Rand Index это RI = 2(a+b)/n(n-1)

То есть это доля объектов, для которых эти разбиения (исходное и полученное в результате кластеризации) "согласованы". Rand Index (RI) выражает схожесть двух разных кластеризаций одной и той же выборки. Чтобы этот индекс давал значения близкие к нулю для случайных кластеризаций при любом и числе кластеров, необходимо нормировать его. ARI принимает значения [-1;1]

/--Adjusted Mutual Information (AMI)--/  
Определяется с использованием функции энтропии, интерпретируя разбиения выборки, как дискретные распределения (вероятность отнесения к кластеру равна доле объектов в нём).

/--Гомогенность, полнота, V-мера--/

/--Силуэт--/  
Коэффициент не предполагает знания истинных меток объектов, и позволяет оценить качество кластеризации, используя только саму (неразмеченную) выборку и результат кластеризации.

a — среднее расстояние от данного объекта до объектов из того же кластера, через b — среднее расстояние от данного объекта до объектов из ближайшего кластера (отличного от того, в котором лежит сам объект)  
s = b-a/max(a,b)  

Силуэт показывает, насколько среднее расстояние до объектов своего кластера отличается от среднего расстояния до объектов других кластеров. Данная величина лежит в диапазоне . Значения, близкие к -1, соответствуют плохим (разрозненным) кластеризациям, значения, близкие к нулю, говорят о том, что кластеры пересекаются и накладываются друг на друга, значения, близкие к 1, соответствуют "плотным" четко выделенным кластерам.

/------------------------Метод главных компонент (principal component analysis, PCA)---------------------------/

Помогает сохранить максимум информации в данных с минимальном количестве переменных. Позволяет уменьшить число переменных, выбрав самые изменчивые. Создает нам новые переменные, исходя из старых.  
Помогает бороться с мультиколлинеарностью.

Метод главных компонент (principal component analysis, PCA) — это один из методов обучения без учителя, который позволяет сформировать новые признаки, являющиеся линейными комбинациями старых.  
При этом новые признаки строятся так, чтобы сохранить как можно больше дисперсии в данных. Иными словами, метод главных компонент понижает размерность данных оптимальным с точки зрения сохранения дисперсии способом.  
Решает задачу понижения размерности - он находит новые признаки, зависящие от старых. При этом целевая переменная никак не используется.  

Сумма весов любой главной компоненты должна равняться единице.

Свойства главных компонент:  
 - Помогают визуализировать сложный набор данных. Допустим у нас есть 1000 параметров и использовав pca мы получим 2-3 главных компонеты в которых будет 90 процентов дисперсии.  
 - Увидеть самые информативные переменные.  
 - Переход к некоррелированным переменным.  
 - Увидеть особенные наблюдения. Если какая-то компонента будет принимать не похожее на другие значение.  
/------------PCA--------------/  
Помогает сохранить максимум информации в данных с минимальном количестве переменных. Позволяет уменьшить число переменных, выбрав самые изменчивые. Создает нам новые переменные, исходя из старых.  
Помогает бороться с мультиколлинеарностью.

Вы забыли упомянуть, что PCA (как и другие классические методы редукции многомерных данных) ищет только линейные зависимости, в отличие от тех же нейронных сетей, которые могут находить и нелинейные зависимости. Это очень важный аспект.

PCA можно сделать через SVD или через матрицу ковариаций

Метод главных компонент (principal component analysis, PCA) — это один из методов обучения без учителя, который позволяет сформировать новые признаки, являющиеся линейными комбинациями старых.  
При этом новые признаки строятся так, чтобы сохранить как можно больше дисперсии в данных. Иными словами, метод главных компонент понижает размерность данных оптимальным с точки зрения сохранения дисперсии способом.  
Решает задачу понижения размерности - он находит новые признаки, зависящие от старых. При этом целевая переменная никак не используется.  

Сумма весов любой главной компоненты должна равняться единице.

Свойства главных компонент:  
 - Помогают визуализировать сложный набор данных. Допустим у нас есть 1000 параметров и использовав pca мы получим 2-3 главных компонеты в которых будет 90 процентов дисперсии.  
 - Увидеть самые информативные переменные.  
 - Переход к некоррелированным переменным.  
 - Увидеть особенные наблюдения. Если какая-то компонента будет принимать не похожее на другие значение.

Снижения размерности данных 2 причины использования:  
- Можно рассматривать как помощь в визуализации данных, для этого часто используется метод t-SNE  
- Может убрать лишние сильно скоррелированные признаки у наблюдений и подготовить данные для дальнейшей обработки в режиме обучения с учителем.

у PCA ограничение – он находит только линейные комбинации исходных признаков  

/--------------Обучение без учителя--------------/

Под этим понимаются ситуации, в которых нужно найти структуру в данных или произвести их "разведку".  
В этом модуле мы обсудим две таких задачи: кластеризацию (поиск групп схожих объектов) и визуализацию (отображение объектов в двух- или трехмерное пространство).

Кластеризация - обучение без учителя.  
Упрощение представления о большом количестве объектов, понимание их структуры.  
разделение множества входных векторов на группы (кластеры) по степени «схожести» друг на друга.

Типы кластерных структур:  
 - Шарообразные  
 - Шарообразные с перемычками  
 - Ленточные  
 - Перекрывающиеся  
 - Кластеры могут отсутствовать  
   
Есть множество мер расстояния, рассмотрим несколько из них:

- Евклидово расстояние — наиболее распространенное расстояние. Оно является геометрическим расстоянием в многомерном пространстве.  
- Квадрат евклидова расстояния. Иногда может возникнуть желание возвести в квадрат стандартное евклидово расстояние, чтобы придать большие веса более отдаленным друг от друга объектам.  
- Расстояние городских кварталов (манхэттенское расстояние). Это расстояние является просто средним разностей по координатам. В большинстве случаев эта мера расстояния приводит к таким же результатам, как и для обычного расстояния Евклида. Однако отметим, что для этой меры влияние отдельных больших разностей (выбросов) уменьшается (так как они не возводятся в квадрат).  
- Расстояние Чебышева. Это расстояние может оказаться полезным, когда желают определить два объекта как «различные», если они различаются по какой-либо одной координате (каким-либо одним измерением).  
- Степенное расстояние. Иногда желают прогрессивно увеличить или уменьшить вес, относящийся к размерности, для которой соответствующие объекты сильно отличаются. Это может быть достигнуто с использованием степенного расстояния.  

Вместо однозначного ответа на вопрос к какому кластеру относится объект, он определяет вероятность того, что объект принадлежит к тому или иному кластеру.  
   
 Методы кластеризации:  
   
 - К-средних (k-means). На вход подаются объекты и количество кластеров k. Положение центра кластера вычисляется как среднее всех объектов данного кластера.  
 - EM-алгоритм (мягкий вариант К-средних).  
  - Иерархическая кластеризация - Помогает определить число кластеров. В отличие от K-means, в случае иерархической кластеризации необходимо вычислять расстояния между кластерами.  
   
 - Агломеративные методы - объеденяют мелкие кластеры в крупные группы и отображают историю отображений в виде дендрограммы.  
 - Дивидивные методы - дробит выборку на мелкие кластеры  
   
 Агломеративные методы:  
 - алгоритм Ланса-Уильямса  
   
/----Алгоритм k-means (k-средних)----/

Наиболее простой, но в то же время неточный метод кластеризации в классической реализации.  
Он разбивает множество элементов векторного пространства на заранее известное число кластеров k.  
Действие алгоритма таково, что он стремится минимизировать среднеквадратичное отклонение на точках каждого кластера.  
Основная идея заключается в том, что на каждой итерации перевычисляется центр масс для каждого кластера, полученного на предыдущем шаге, затем векторы разбиваются на кластеры вновь в соответствии с тем, какой из новых центров оказался ближе по выбранной метрике.  
Алгоритм завершается, когда на какой-то итерации не происходит изменения кластеров.

Проблемы алгоритма k-means:  
- необходимо заранее знать количество кластеров.  
- алгоритм очень чувствителен к выбору начальных центров кластеров.  
- не справляется с задачей, когда объект принадлежит к разным кластерам в равной степени или не принадлежит ни одному.

/----Алгоритм c-means (c-средних)----/

/-----------------Понижение размерности-----------------/  
   
 PCA - линейный метод понижения размерности.  
 t-SNE - нелинейный.

то и то можно использовать для разведки данных и визуализации. на этом все:

-у PCA  есть модель, у TSNE нет  
-у PCA можно напрямую делать inference новых точек, у TSNE только хаками  
-PCA линеен, TSNE нет  
-для стандартного PCA есть готовая алгебраическая процедура построения, TSNE только градиентный  
-целевые функционалы PCA и TSNE не имеют вообще ничего общего (edited)

Как можно сбалансировать выборку  
способы простых смертных:  
- undersampling доминирующего класса  
- oversampling минорного класса

способ тех кто знает арифметику:  
- перевзвесить выборку

more-layers  
- взять генеративную модель типа GAN или RBM и нагенерить нужного класса сколько угодн