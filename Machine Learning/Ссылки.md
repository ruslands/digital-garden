[https://www.vanvalen.caltech.edu](https://www.vanvalen.caltech.edu)

[https://www.deepcell.org/predict](https://www.deepcell.org/predict) # Готовое решение

[https://deepcell.readthedocs.io/en/master/notebooks/Training-Tracking.html](https://deepcell.readthedocs.io/en/master/notebooks/Training-Tracking.html)

[https://github.com/pytorch/vision/issues/9](https://github.com/pytorch/vision/issues/9)

[https://pytorch.org/docs/stable/_modules/torch/random.html](https://pytorch.org/docs/stable/_modules/torch/random.html)

[https://stackoverflow.com/questions/46382578/different-pytorch-random-initialization-with-the-same-seed](https://stackoverflow.com/questions/46382578/different-pytorch-random-initialization-with-the-same-seed)

[https://pytorch.org/tutorials/beginner/data_loading_tutorial.html](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)

[https://github.com/pytorch/vision/issues/9](https://github.com/pytorch/vision/issues/9)

[https://habr.com/company/mailru/blog/335164/](https://habr.com/company/mailru/blog/335164/) # satellite competition

[https://www.kaggle.com/c/data-science-bowl-2018/discussion/54577](https://www.kaggle.com/c/data-science-bowl-2018/discussion/54577) # more links

[http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplabv3](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplabv3)

[https://www.slideshare.net/samchoi7/semantic-segmentation-methods-using-deep-learning](https://www.slideshare.net/samchoi7/semantic-segmentation-methods-using-deep-learning)

[http://forums.fast.ai/t/implementing-mask-r-cnn/2234/50](http://forums.fast.ai/t/implementing-mask-r-cnn/2234/50)

[https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html](https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html)

[https://www.yuthon.com/2017/04/27/Notes-From-Faster-R-CNN-to-Mask-R-CNN/](https://www.yuthon.com/2017/04/27/Notes-From-Faster-R-CNN-to-Mask-R-CNN/)

[https://blog.csdn.net/yunxinan/article/details/71374882](https://blog.csdn.net/yunxinan/article/details/71374882)

[https://blog.csdn.net/u010069760/article/details/78465505](https://blog.csdn.net/u010069760/article/details/78465505)

[https://www.ctolib.com/tf-faster-rcnn.html](https://www.ctolib.com/tf-faster-rcnn.html) ([https://arxiv.org/pdf/1702.02138.pdf](https://arxiv.org/pdf/1702.02138.pdf))

[http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf](http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf)

[http://forums.fast.ai/t/kaggle-data-science-bowl-2018-find-and-segment-nuclei/9966](http://forums.fast.ai/t/kaggle-data-science-bowl-2018-find-and-segment-nuclei/9966)

[https://github.com/mrgloom/awesome-semantic-segmentation](https://github.com/mrgloom/awesome-semantic-segmentation)

[https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html)

[https://skrish13.github.io/articles/2018-03/fair-cv-saga](https://skrish13.github.io/articles/2018-03/fair-cv-saga)

[https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/](https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/)

[https://arxiv.org/pdf/1804.04241.pdf](https://arxiv.org/pdf/1804.04241.pdf) # capsule net

[https://arxiv.org/pdf/1706.04737.pdf](https://arxiv.org/pdf/1706.04737.pdf) # A Deep Active Learning Framework for Biomedical Image Segmentation

[https://arxiv.org/abs/1712.04440](https://arxiv.org/abs/1712.04440) # Data distillation

[https://www.kaggle.com/c/data-science-bowl-2018#evaluation](https://www.kaggle.com/c/data-science-bowl-2018#evaluation)

[https://arxiv.org/pdf/1802.05591.pdf](https://arxiv.org/pdf/1802.05591.pdf)

[https://www.kaggle.com/wcukierski/example-metric-implementation/](https://www.kaggle.com/wcukierski/example-metric-implementation/)

[https://arxiv.org/abs/1611.10012](https://arxiv.org/abs/1611.10012) # прочиать обязательно

На основе этой статьи сложилось впечатление, что лучшие по точности архитектуры — на основе Faster R-CNN,

а по скорости — на SSD. Поэтому в продакшен пилят SSD, а для соревнований и так далее — Faster R-CNN.

[https://github.com/albu/albumentations/blob/master/notebooks/example_kaggle_salt.ipynb](https://github.com/albu/albumentations/blob/master/notebooks/example_kaggle_salt.ipynb) # albumentations for segmentation

# In this document we will perform two types of transfer learning: finetuning and feature extraction.

[https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) # finetuning

[https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5](https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5)

[https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)

[https://hai.stanford.edu/research/ai-index-2022?fbclid=IwAR1KVCeJ6Ehqb8vi94TzxDcg4aFxQf7WlTlESgMA4IbEo79ItrEWJnA3LIc](https://hai.stanford.edu/research/ai-index-2022?fbclid=IwAR1KVCeJ6Ehqb8vi94TzxDcg4aFxQf7WlTlESgMA4IbEo79ItrEWJnA3LIc)

[https://www.fast.ai](https://www.fast.ai)

# Дело в том, что в нашем случае мы хотим использовать dice coefficient как функцию потерь вместо того,

# что они называют «энергетической функцией», так как это показатель, используемый в соревновании Kaggle,

# который определяется: X является нашим предсказанием и Y - правильно размеченной маской на текущем объекте.

# |X| означает мощность множества X (количество элементов в этом множестве) и ∩ для пересечения между X и Y.

COCO - Common Objects in Context

torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs.

===transfer learning===

Определим цели transfer learning:

higher start — улучшение качества обучения уже на начальных итерациях за счет более тщательной подборки начальных параметров модели или какой-либо другой априорной информации;

higher slope — ускорение сходимости алгоритма обучения;

higher asymptote — улучшение верхней достижимой границы качества.

Если вы знакомы с предобучением глубоких сетей с помощью автоенкодеров или ограниченных машин Больцмана

Представьте, что есть две задачи, и решали их, возможно, даже разные люди. Один из них использует часть модели другого (source task) для уменьшения временных затрат на создание модели с нуля и улучшения производительности своей модели (target task). Процесс передачи знаний от одной проблемы к другой и есть transfer learning.

Другой особенностью transfer learning является то, что информация может передаваться только из старой модели в сторону новой, ведь старая задача уже давно решена.

Трансфер знаний можно также рассматривать как некоторую регуляризацию, которая ограничивает пространство поиска до определенного набора допустимых и хороших гипотез.

Using a target size (torch.Size([4, 3, 160, 160]))

that is different to the input size (torch.Size([4, 1, 160, 160])) is deprecated.

Please ensure they have the same size. Target nelement (307200) != input nelement (102400)

Я на работе убил два рабочих дня и перебил все эти точечки под bounding boxes.

Прямо взял sloth и вручную перебил train set

До этого я использовал исключительно Keras с бэкендом от Theana или TensorFlow.

Framework MxNet - good for parallelization

Island segregation

используем Faster R-CNN, у которой base был VGG-16

Сейчас на Kaggle — да и везде — достаточно часто проходит трюк, когда добавляется дополнительная информация,

дополнительные метки к тренировочным данным. И это позволяет гордо въехать в топ.

Много статей про то, как работать и с bounding boxes, и с точками.

В принципе, когда вы предсказываете и классифицируете object detection через bounding box,

точность выше просто потому, что надо предсказать координаты этого четырехугольника.

метрика Jaccard

используя Faster RCNN, подтвердил, что на практике, VGG как feature extractor работает лучше.

1. Семейство two stage detectors: R-FCN, Fast-RCNN, Faster-RСNN — то есть сети, у которых pipeline

можно разделить на две части: первая — найти области на картинке, которые соответствуют объектам,

а вторая — провести классификацию для каждой области, чтобы определить какой именно объект там содержится.

2. Семество one stage detectors: YOLO, YOLO9000, SSD — в этих архитектурах поиск областей и

классификация происходит в один заход.

skimage.measure.label #Label connected regions of an integer array. It takes the mask and performs connected component

labeling in order to show the different masks and generate them independently.

skimage.morphology.remove_small_objects(ar) # Remove objects smaller than the specified size.

i modify the algorithm to detect "cuts" instaed of the boundary

U-net still provides outstanding results when the

input image of each convolutional layer is padded with zeros of width one.

In this case, the input resolution matches the output image resolution.

With this model, we ignore that convolutions at the image boundary

are strictly speaking not valid.

RMSprop optimizer with a learning rate of 10−4 worked best for all models

Intersection over Union (IoU) for object detection

For the 3-class formulation, we add a convolutional layer with three layers

and a softmax activation the ensure the validity of the predicted probability distribution.

For the boundary formulation, we add a convolutional layer with a single

channel and the sigmoid function as an activation.

The output of the annotation tool is a mask that assigns each pixel to a class.

Background pixels were annotated with 0, pixels belonging to nuclei were annotated

with positive integers in such a way that touching nuclei were assigned different numbers

Submissions

[https://www.kaggle.com/weiji14/yet-another-keras-u-net-data-augmentation/comments](https://www.kaggle.com/weiji14/yet-another-keras-u-net-data-augmentation/comments)

[https://www.kaggle.com/kmader/optimizing-computer-vision-segmentation/comments](https://www.kaggle.com/kmader/optimizing-computer-vision-segmentation/comments)

About cell Profiler ImageJ

[https://www.kaggle.com/c/data-science-bowl-2018/discussion/47665](https://www.kaggle.com/c/data-science-bowl-2018/discussion/47665)

semantic segmentation and instance segmentation

Натренировали сеть. Как делать предсказания? Изначально 2000 на 2000 режутся на куски 1000 на 1000 с перехлестом.

Опять же, можно поизвращаться и сделать такой фокус — предсказать, например, на кусочке машинки, а потом взять

этот изначальный спутниковый снимок, как-то отразить, сделать предсказание и потом отразить обратно.

По сути, мы предсказываем то же самое в первом и втором случае, но немножко по-разному.

Можно считать это некой ансамблевой техникой. И — алгоритм non-maximum suppression.

По сути, если мы порезали картинку 2000 на 2000 на куски с перехлестом, то одна машинка может оказаться в

различных кусках. Когда мы предсказываем, получается два предсказания одной и той же машинки.

Это надо чистить — что и делает алгоритм non-maximum suppression. Если два bounding box пересекаются

в предсказаниях, тот, у которого confidence повыше, остается, а остальное режется под ноль.

Задачки про вероятность, особенное любят задачки на теорему Байеса.

mean average precision or “mAP score” Evaluation Metric for Object Detection

функция времени

Метод Оцу использует гистограмму изображения для расчета порога.

Решение сводится к минимизации внутриклассовой дисперсии

padding(same,valid), stride

print(model) # print network architecture

=========Train, Validation, Test==========

The training and validation sets are used during training.

for each epoch

    for each training data instance

        propagate error through the network

        adjust the weights

        calculate the accuracy over training data

    for each validation data instance

        calculate the accuracy over the validation data

    if the threshold validation accuracy is met

        exit training

    else

        continue training

Once you're finished training, then you run against your testing set and verify that the accuracy is sufficient.

Training Set: this data set is used to adjust the weights on the neural network.

Validation Set: this data set is used to minimize overfitting.

    You're not adjusting the weights of the network with this data set,

    you're just verifying that any increase in accuracy over the training data

    set actually yields an increase in accuracy over a data set that has not been

    shown to the network before, or at least the network hasn't trained on it (i.e. validation data set).

    If the accuracy over the training data set increases, but the accuracy over the validation data

    set stays the same or decreases, then you're overfitting your neural network and you should stop training.

Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.

    ==

The error surface will be different for different sets of data from your data set (batch learning).

Therefore if you find a very good local minima for your test set data, that may not be a very good point,

and may be a very bad point in the surface generated by some other set of data for the same problem.

Therefore you need to compute such a model which not only finds a good weight configuration for the training

set but also should be able to predict new data (which is not in the training set) with good error.

In other words the network should be able to generalize the examples so that it learns the data and does

not simply remembers or loads the training set by overfitting the training data.

The validation data set is a set of data for the function you want to learn, which you are not directly

using to train the network. You are training the network with a set of data which you call the training data set.

If you are using gradient based algorithm to train the network then the error surface and the gradient at

some point will completely depend on the training data set thus the training data set is being directly

used to adjust the weights. To make sure you don't overfit the network you need to input the validation

dataset to the network and check if the error is within some range. Because the validation set is not

being using directly to adjust the weights of the netowork, therefore a good error for the validation

and also the test set indicates that the network predicts well for the train set examples, also it is

expected to perform well when new example are presented to the network which was not used in the training process.

Early stopping is a way to stop training. There are different variations available, the main outline is,

both the train and the validation set errors are monitored, the train error decreases at each iteration

(backprop and brothers) and at first the validation error decreases. The training is stopped at the

moment the validation error starts to rise. The weight configuration at this point indicates a model,

which predicts the training data well, as well as the data which is not seen by the network .

But because the validation data actually affects the weight configuration indirectly to select

the weight configuration. This is where the Test set comes in. This set of data is never used in the

training process. Once a model is selected based on the validation set, the test set data is applied

on the network model and the error for this set is found. This error is a representative of the error

which we can expect from absolutely new data for the same problem.

Also, in the case you do not have enough data for a validation set, you can use crossvalidation to

tune the parameters as well as estimate the test error.

====

For example, there are tools for single-cell analysis of bacteria (SuperSegger21, Oufti22, Morphometrics23),

24,25 26 single-cell analysis of mammalian cells (CellProfiler , Ilastik ,

Microscopy Image Browser27), and general-purpose image analy- sis (ImageJ28, OMERO29).

Van Valen, D. A. et al. Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments. PLOS Comput. Biol. 12, e1005177 (2016).

Object-detection-based deep learning methods have also been adapted for instance segmentation. These methods, which include Faster R-CNN89 and Retinanet90, predict bounding boxes for all objects in an image and use non-maximum suppression to remove redundant bounding-box predictions. Mask R-CNN91, one of the most accurate methods for instance segmen- tation on general-purpose datasets, builds on these methods to predict an object mask for each bounding box.

Jaccard index: a metric for segmentation accuracy180; intersection over union. Mathematically, if R is a reference segmentation and S is a predicted segmentation, then the Jaccard index is given by R∩S∕R∪S.

Dice index: a metric for segmentation accuracy180, defined mathematically as 2 ∣R∩S∣∕(∣R∣ + ∣S∣).

We recommend that users focus their initial efforts on the existing DeepCell, CDeep3M, U-Net, CellProfiler, or Mask R-CNN software libraries, as these methods have been applied successfully to a variety of data types, provide pretrained models, and support both model training and deployment on new data.

===Optimizers===

[https://habr.com/en/post/318970/](https://habr.com/en/post/318970/)

Momentum is a variation of the stochastic gradient descent used for faster convergence of the loss function.

скормим целевую функцию(loss function) оптимизатору(optimizer)

NN optimizers:

Adadeltaи

Adagrad

Adam

Adamax

ASGD

LBFGS

lr_scheduler

Optimizer

RMSprop

Rprop

SparseAdam

A torch model normally expects a batch of images as input.

If you want to pass a single image, make sure it is still a batch of single image.

Usually you would iterate over the DataLoader, not the Dataset.

If Dataset iteration

inputs = inputs.unsqueeze(0)

outputs = net(inputs)

If Dataloader iteration

for i, data in enumerate(train_data_loader):

    ...

You have to shape your input to this format (Batch, Number Channels, height, width).

Currently you have format (B,H,W,C) (4, 32, 32, 3), so you need to swap 4th and 2nd axis to shape your data with

(B,C,H,W). You can do it this way:

Annotation Summary of image segmentation.pdf.

Introduction

Highlight [1]: Morphological proﬁling as a method of systems biology aims at quantifying changes in the structure of cells as a result of biological perturbations

Highlight [1]: CellProﬁler is a commonly used state-of-the-art software tool for the segmentation of ﬂuorescence microscopy images, and the extraction of single-cell proﬁles

Highlight [1]: The neural network is trained to predict the probability of belonging to the

Highlight [2]: outline of a nucleus for each pixel of the input image.

Highlight [2]: To evaluate the performance of our neural network, we refrain from using pixel-based metrics

Highlight [2]: As computing object-based metrics such as the share of undetected nuclei is more relevant to biologists, we adhere to metrics common in object detection

Highlight [2]: However, models trained on hand-annotated images surpass the performance of CellProﬁler

High-Content and High-Throughput Analysis

Highlight [4]: Fluorescence microscopy can combine the advantages of high-throughput and high-content analysis

Analysis Pipeline

Highlight [12]: Dye name Hoechst 33342 Concanavalin A (Alexa Fluor 488) SYTO TM 14 Phalloidin (Alexa Fluor 568) WGA (Alexa Fluor 555) MitoTracker Deep Red

Excitation 387 472 531 562 562 628

Emission 417-477 503-538 573-613 622-662 622-662 672-712

Component Hoechst 33342 Concanavalin A (Alexa Fluor 488) SYTO TM 14 Phalloidin (Alexa Fluor 568) WGA (Alexa Fluor 555) MitoTracker Deep Red

Excitation 387 Concanavalin A (Alexa Fluor 488) SYTO TM 14 Phalloidin (Alexa Fluor 568) WGA (Alexa Fluor 555) MitoTracker Deep Red

Excitation 387 472 SYTO TM 14 Phalloidin (Alexa Fluor 568) WGA (Alexa Fluor 555) MitoTracker Deep Red

Excitation 387 472 531 Phalloidin (Alexa Fluor 568) WGA (Alexa Fluor 555) MitoTracker Deep Red

Excitation 387 472 531 562 WGA (Alexa Fluor 555) MitoTracker Deep Red

Excitation 387 472 531 562 562 MitoTracker Deep Red

Excitation 387 472 531 562 562 628 417-477 503-538 573-613 622-662 622-662 672-712

Component Nucleus 503-538 573-613 622-662 622-662 672-712

Component Nucleus Endoplasmic reticulum 573-613 622-662 622-662 672-712

Component Nucleus Endoplasmic reticulum Nucleoli, cytoplasmic RNA 622-662 622-662 672-712

Component Nucleus Endoplasmic reticulum Nucleoli, cytoplasmic RNA F-actin cytoskeleton 622-662 672-712

Component Nucleus Endoplasmic reticulum Nucleoli, cytoplasmic RNA F-actin cytoskeleton Plasma membrane 672-712

Component Nucleus Endoplasmic reticulum Nucleoli, cytoplasmic RNA F-actin cytoskeleton Plasma membrane Mitochondria

Instance Segmentation

Highlight [14]: instance segmentation assigns pixels to object

Image Segmentation in CellProfiler

Highlight [15]: It classiﬁes pixels into foreground and background pixels and then assigns the foreground pixels to an object instance.

Highlight [15]: CellProﬁler’s current implementation for segmentation is implemented in the module IdentifyPrimaryObjects and consists of three steps:

Image Understanding

Highlight [26]: In the ﬁrst formulation, we classify each pixel as either nucleus, boundary or background pixel with multinomial classiﬁcation

Highlight [26]: We call this formulation the 3-class formulation

Adaptions to U-net

Highlight [27]: The output of our model has three channels in our 3-class formulation and one channel in our boundary formulation.

Highlight [27]: U-net still provides outstanding results when the input image of each convolutional layer is padded with zeros of width one. In this case, the input resolution matches the output image resolution.

Highlight [29]: With this model, we ignore that convolutions at the image boundary are strictly speaking not valid.

Highlight [29]: Batch normalization is used after each convolution as a regularization technique and to avoid the vanishing gradient problem. A momentum value for the moving average computation of 0.9 is used

Segmentation with CellProfiler

Highlight [32]: In addition, we want to use CellProﬁler to generate training data for the deep learning model.

Implementation

Highlight [40]: To compare segmentation algorithms across data sets, we consider the algorithms’ task as object detection and thus calculate object-level precision and recall metrics [72]. Precision represents the share of matches among all detected objects, the recall measures the share of matches among all nuclei in the ground truth annotation:

Experiments and Results

Highlight [42]: we present the results obtained with deep learning models that were trained on CellProﬁler segmentations of the training images. As these segmentations are considered noisy, we improve our model by training on hand-annotations.

Learning on Automatically Generated Annotations

Highlight [46]: We formulate the segmentation problem as a 3-class problem introduced in section 3.1.1 which predicts the probability of each pixel belonging to either a nucleus, a nucleus’ boundary or the image’s background.

Highlight [46]: The segmentations generated by CellProﬁler are preprocessed in such a way that the boundaries around nuclei have a width of 2 pixels.

Highlight [46]: As the network predicts a probability distribution across the three classes for each pixel, we use the class for which the network predicted the highest probability.

Highlight [46]: A further optimization that can be performed is dilating the detected nuclei by half of the boundary width.

Highlight [46]: This model uses the softmax function introduced in section 2.4.3 as the activation function for the last layer and categorical cross-entropy as the loss function.

Highlight [46]: We want to mention here, that both, categorical cross-entropy and classiﬁcation accuracy are pixel-based metrics and not object-based, even though object-based metrics are more relevant to biologists.

Highlight [47]: Pixel-wise

Highlight [47]: We see that classiﬁcation accuracy thus is not a good metric for the performance of the model as discussed in section

Highlight [47]: Training this network takes roughly two hours for 200 epochs.

Predicting Nuclei Outlines

Highlight [51]: Common errors occurring with this approach are merge errors which are due to the lack of boundary pixels between clumped nuclei. To resolve these errors, predicting only the outline of nuclei is a promising approach.

Highlight [51]: In addition, with the previously discussed models, we observed that the CNN had no diﬃculties with classifying background and nucleus pixels correctly. Errors were mainly due to the misclassiﬁcation of boundary pixels. Thus, we force the network to focus on outlines of nuclei only by removing the other classes and using the boundary formulation discussed in section

Highlight [51]: Thus, in this and the following experiment, the probability for a pixel to belong to an outline of a nucleus is predicted.

Reformulation of the Segmentation Problem

Highlight [51]: Outline Prediction as Classiﬁcation Problem. As the boundary pixels are not naturally present in the data, but artiﬁcially generated, we tried multiple diﬀerent variants for generating these. Instead of generically having a boundary width of two pixels as in the previous experiments, we consider experiment setups with boundary widths of two, four, six and eight pixels. The skimage.morphology library was used for that purpose

Highlight [52]: Outline Prediction as Regression Problem. In addition to this classiﬁcation problem, we also framed the outline prediction formulation as a regression model. In this problem, we allowed the annotations to be continuous instead of categorical and thus represent a probability with which a pixel belongs to a nucleus’ outline. The closer a given pixel is to the boundary, the higher we set its probability to belong to it.

Highlight [52]: This boundary was created in a preprocessing step as a linear combination of the boundaries with two, four, six and eight pixels. The coeﬃcients of the linear combinations were chosen in such a way that the probability of a pixel belonging to the outline follows a Gaussian bell in the distance of the pixel to the boundary. It is thereby assumed that the boundary is in between the two pixels of the boundary of width two. The Gaussian bell was scaled such that the two pixels adjoint to the boundary had a probability of 1 to belong to the boundary. For this model, we keep the setup as previously deﬁned for the classiﬁcation problem. The only diﬀerence is that the mean squared error is used as a loss function.

Getting Segmentations from Outlines

Highlight [53]: A second method we considered is ﬁnding connected components among pixels that are not classiﬁed as boundary and ﬁltering the largest connected component as background.

Highlight [53]: This method was unsatisfactory because it failed for cases where the background was not connected, which is often the case at the image’s boundaries.

Improved Segmentations with Outline Prediction

Highlight [54]: We observe a better performance for the models with higher boundary width. Their segmentations do not show many unclosed contours. Among the thick boundaries, the model using a thickness of 4 delivered the best segmentations

Highlight [56]: Besides being more complex, the model treating the outline detection problem as a regression problem performs worse than the models trained with a categorical boundary.

Highlight [56]: Thus we drop this approach and focus on optimizing the outputs obtained with the model using a boundary width of four pixels in the next section.

Improving Performance with Data Augmentation

Highlight [56]: However, a common error on the test set for this models is the misclassiﬁcation of boundary pixels between clumped nuclei as pixels belonging to a nucleus

Improving Models in 3-Class Formulation

Highlight [64]: We thus conclude that training the CNN with boundary formulation and using data augmentation gives us the best results.

Conclusion

Highlight [68]: A method commonly used in object detection was used for comparing segmentations for ﬂuorescence microscopy images. It is an object-based measure and thus much more relevant to biologists than a pixel-based comparison. We used this method to evaluate the performance of diﬀerent deep learning models for segmentations.

Highlight [68]: Clumped nuclei are common in ﬂuorescence microscopy and thus separating touching nuclei is crucial for segmentation algorithms. By introducing an artiﬁcial boundary class between touching nuclei and between nuclei and background pixels, we could separate clumped nuclei using semantic segmentation with convolutional neural networks. The fully convolutional networks we used here allow end-to-end training and can do predictions on an arbitrary input size.

Highlight [68]: The neural networks we trained easily surpass the performance of seeded watershed algorithms used with a CellProﬁler segmentation pipeline designed by experts.

Outlook

Highlight [69]: networks need to yield segmentations with even fewer merge errors.

Highlight [69]: Thus, more research should be conducted on cost functions that put more weight on areas where clumped nuclei are depicted in the input image.

Highlight [69]: In addition, using object detection approaches or instance segmentation instead of semantic segmentation as discussed in this thesis, might yield better results. However, these models are signiﬁcantly harder to implement and train.